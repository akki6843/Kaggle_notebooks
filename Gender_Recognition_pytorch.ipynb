{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8899631,"sourceType":"datasetVersion","datasetId":5328253}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-30T18:33:30.316205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nfrom matplotlib import pyplot as plt\n\nimport torch\nimport torchvision\nimport torch.nn as nn\n\nfrom torch.utils.data import Dataset\nfrom torchvision.io import read_image\nfrom torch.utils.data import DataLoader\n\nfrom torchvision import transforms \nfrom sklearn.model_selection import train_test_split\n\nimport torchinfo","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_loc = \"/kaggle/input/gender-recognizer/dataset\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.listdir(dataset_loc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"men_data = os.path.join(dataset_loc, \"MEN\")\nwomen_data = os.path.join(dataset_loc, \"WOMAN\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(men_data)\nprint(women_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_df = pd.DataFrame(columns=[\"filename\", \"label\"])\nfor d in tqdm(os.listdir(men_data)):\n    df = pd.DataFrame({\"filename\":os.path.join(men_data, d), \"label\":\"MEN\"}, index=[0])\n    data_df = pd.concat([data_df, df], ignore_index=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for d in tqdm(os.listdir(women_data)):\n    df = pd.DataFrame({\"filename\":os.path.join(women_data, d), \"label\":\"WOMAN\"}, index=[0])\n    data_df = pd.concat([data_df, df], ignore_index=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_df.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_df=data_df.sample(frac=1.0).reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_df.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in data_df[\"filename\"]:\n    if  (not i.endswith(\".jpg\")) and (not i.endswith(\".png\")):\n        print(i)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_df = data_df[data_df[\"filename\"].str.endswith((\".jpg\", \".png\"))]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_transforms = transforms.Compose([\n    transforms.Resize((128, 128)),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomAutocontrast(),\n    transforms.RandomAdjustSharpness(2),\n])\n\n# Only reshape test data\ntest_transforms = transforms.Compose([\n    transforms.Resize((128, 128)),\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GenderRecognitionDataset(Dataset):\n    def __init__(self, dataset_df, transform):\n        self.dataset_df = dataset_df\n        self.filenames = self.dataset_df[\"filename\"].to_list()\n        self.labels = self.dataset_df[\"label\"].to_list()\n        self.classes = sorted(list(dataset_df['label'].unique()))\n        self.class_to_idx = {cls_name: _ for _, cls_name in enumerate(self.classes)}\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.dataset_df)\n    \n    def __getitem__(self, idx):\n        self.filename = self.filenames[idx]\n        image = read_image(self.filename)\n        image = image/255.0\n        image = self.transform(image)\n        label = self.labels[idx]\n        return image, label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gdr = GenderRecognitionDataset(data_df, transform=train_transforms)\nimg, lab = next(iter(gdr))\nplt.imshow(img.permute(1,2,0))\nplt.title(lab)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(gdr, batch_size=6, shuffle=True)\nsample_images, sample_labels = next(iter(train_loader))\nfor img, label in zip(sample_images, sample_labels):\n    plt.imshow(img.permute(1,2,0))\n    plt.title(label)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df, test_df = train_test_split(data_df, test_size=0.20, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = GenderRecognitionDataset(train_df, train_transforms)\ntest_data = GenderRecognitionDataset(test_df, test_df)\n\ntrain_dataLoader = DataLoader(train_data, batch_size=8, shuffle=True)\ntest_dataLoader = DataLoader(test_data, batch_size=1, shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GenderNet(nn.Module):\n    def __init__(self):\n        super().__init__()      \n        self.fetaure_extraction = nn.Sequential(\n            nn.Conv2d(3,16,kernel_size=3),\n            nn.ELU(),\n            nn.MaxPool2d(kernel_size=2),\n            nn.Conv2d(16, 32, kernel_size=3),\n            nn.ELU(),\n            nn.MaxPool2d(kernel_size=2),\n            nn.Flatten()\n        )\n        self.classifier = nn.Sequential(\n            nn.Linear(123008, 8),\n            nn.ReLU(),\n            nn.Linear(8, 1),\n            nn.Sigmoid()\n        )\n    \n    def forward(self, x):\n        x = self.fetaure_extraction(x)\n        print(x.shape)\n        x = self.classifier(x)\n        return x\n    \n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = GenderNet().to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torchinfo.summary(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(1):\n    for images, labels in train_dataLoader:\n        optimizer.zero_grad()\n        predictions = model(images)\n        losses = criterion(predictions, labels)\n        losses.backward()\n        optimizer.step()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}